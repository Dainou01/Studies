% !TEX program = xelatex
\documentclass[compress]{beamer}

\usepackage[english]{babel}
\usepackage{metalogo}
\usepackage{listings}
\usepackage{fontspec}
\usepackage{amsmath, amssymb}
\usepackage{stackrel}
\usepackage{tikz}
\usepackage{svg}
\usepackage{unicode-math}
\usepackage[theme=nord,charsperline=60,linenumbers]{jlcode}

\usetheme{Nord}

\setmainfont{Roboto}
% \setsansfont{DejaVu Serif}
\setmonofont{JuliaMono}

\newcommand{\E}[1]{\ensuremath{E\left\{#1\right\}}}

\AtBeginSection[]
{
    \begin{frame}[c,noframenumbering,plain]
        \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/show/hide]
    \end{frame}
}


\AtBeginSubsection[]
{
    \begin{frame}[c,noframenumbering,plain]
        \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/shaded/hide]
    \end{frame}
}


\title{Project I: Random Processes}
\subtitle{Random variables, stationarity \& ergodicity}
\author{Simon Andreas Bjørn}
\date{February 7, 2023}

\begin{document}

\begin{frame}[plain,noframenumbering]
    \maketitle
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Random variables}
    Importing some statistic and plotting modules
    \begin{jllisting}[gobble=8]
        using Printf
        using Statistics
        using NaNStatistics
        using Plots
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1.1 Random variables - uniform PDF}
    We start by sampling the uniform destribution on $\left[0,1\right)$,
    with $N=10000$ samples.
    \begin{jllisting}[gobble=8]
        N = 10_000
        x = rand(N)
    \end{jllisting}
    \pause
    We then find the first order momentum $m_x = E\{x\}$ and
    the second order central moment $\sigma_x^2 = E\{(x-m_x)^2\}$
    \begin{jllisting}[gobble=8]
        # Evaluate E{x} and E{(x-mₓ)²} by definition
        mₓ  = sum(x*1/N)
        σₓ² = sum((x.-mₓ).^2 * 1/N)

        # Evaluate E{x} and E{(x-mₓ)²} by builtin functions
        mₓ_builtin = mean(x)
        σₓ²_builtin = std(x)^2
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Calculation differences}
    Calculating the difference between the builtin functions and our direct evaluation, we 
    see that the the two methods are similar.
    \begin{jllisting}[gobble=8]
        @printf "mₓ_err: %.4f" abs(mₓ-mₓ_builtin)
        # -> mₓ_err: 0.0000
        @printf "σₓ²_err: %.4f" abs(σₓ²-σₓ²_builtin)
        # -> σₓ²_err: 0.0000
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Calculating the PDF}
    We then calculate the pdf using \jlinl{histcounts}, and normalize the PDF.
    \begin{jllisting}[gobble=8]
        step_size = 0.05
        edges = -0.2:step_size:1.2
        bins = histcounts(x, edges)
        
        # Normalize bins to get PDF
        pdf = bins/sum(bins*step_size)

        # Theoretical PDF
        theoretical_pdf = (x) -> Float64(0 <= x < 1)
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code for plotting the PDF}
    And we plot the result
    \begin{jllisting}[gobble=8]
        plot(
            edges[1:end-1],
            pdf,
            seriestype=:steppost, 
            label="Estimated PDF"
        )
            plot!(
            edges[1:end-1],
            theoretical_pdf.(edges[1:end-1]),
            seriestype=:steppost, label="Theoretical PDF"
        )
            plot!(
            legend=:bottom,
            background_color=:transparent,
            foreground_color=:white
        )
        title!("PDF of x");xlabel!("Value");
        ylabel!("Probability");savefig("uniform_pdf.svg")
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{Plot of the PDF}
    \begin{figure}
        \includesvg[width=\columnwidth]{"../uniform_pdf.svg"}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ensuring properties}
    We want to verify if our PDF fulfill the 2 following properties: 
    \begin{enumerate}
        \item $\forall \alpha, \ f_x(\alpha) \ge 0$ \\
            \jlinl{reduce(&, pdf .>= 0) # -> true}
        \item $\int_{-\infty}^{\infty} f_x(\alpha) \; \texttt{d}\alpha = 1$ \\ 
            \jlinl{abs(sum(pdf*step_size)-1.0) < 1e-12 # -> true}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Deriving the theoretical mean [$m_x$]}
    \pause
    We start at the definition of the mean value which is given by
    \begin{equation}
        m_x = E\{x\} = \int_{-\infty}^{\infty}\alpha f_x(\alpha)\texttt{d}\alpha
    \end{equation}
    \pause
    We then use the theoretical probability density function of a uniform distribution
    which is given by 
    \begin{equation*}
        f_x(\alpha) = 
        \begin{cases}
            0, & \alpha < a \\
            \frac{1}{b-a}, & a \le \alpha \le b \\
            0, & \alpha > b 
        \end{cases}
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Deriving the theoretical mean [$m_x$]}
    With $a = 0, b = 1$, we get the theoretical PDF
    \begin{equation*}
        f_x(\alpha) = 
        \begin{cases}
            0, & \alpha < 0 \\
            1, & 0 \le \alpha \le 1 \\
            0, & \alpha > 1 
        \end{cases}
    \end{equation*}
    \pause
    We simplify eq.1, and get the integral
    \begin{equation*}
        m_x = E\{x\} = \int_0^1\alpha \ \texttt{d}\alpha
    \end{equation*}
    \pause
    We simply solve this and get the mean value to be
    \begin{equation*}
        m_x = \frac{1}{2} \left[\alpha^2\right]_0^1=\frac{1}{2}
    \end{equation*}
\end{frame}
\begin{frame}
    \frametitle{Deriving the theoretical variance [$\sigma_x^2$]}
    \pause
    Again, we start at the definition
    \begin{equation}
        \sigma_x^2=E\left\{(x-m_x)^2\right\}=\int_{-\infty}^{\infty}
        (\alpha - m_x) f_x( \alpha ) \texttt{d}\alpha
    \end{equation}
    \pause
    We do the same simplification as with the mean value, and and fill in for $m_x$.
    This gives the variance
    \begin{align*}
        \sigma_x^2=& \int_0^1\left(\alpha-\frac{1}{2}\right)^2\ \texttt{d}\alpha \\
        =& \int_0^1 \alpha^2-\alpha+\frac{1}{4}\ \texttt{d} \alpha \\
        =& \left[\frac{1}{3}\alpha^3-\frac{1}{2}\alpha^2+\frac{1}{4}\alpha\right]^1_0 \\
        =& \frac{1}{12} \approx 0.08334
    \end{align*}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Repeating the uniform experiment}
    We repeat the experiment, find the mean and variance of each experiment and
    accumulate the results.
    \begin{jllisting}[gobble=8]
        r = 50 # Repeat r times
        xs = rand(Float64, (N,r))
        ms = mean(xs, dims=1)
        σs = std(xs, dims=1).^2
        # Accumulate the results
        i_mean = [mean(ms[1:i]) for i in 1:length(ms)]
        i_std = [mean(σs[1:i])^2 for i in 1:length(s)]
        # Plot the results
        plot(1:r, i_mean, label="Cumulative mean", 
            legend=:bottomleft, xlabel="Experiment iteration",
            ylabel="Mean value")
        plot!( twinx(), i_std, label="Variance",
            legend=:bottomright, color=:orange, ylabel="Variance")
        plot!(foreground_color=:white, 
            background_color=:transparent)
        savefig!("repeated_experiment_uniform.svg")
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{Repeating the uniform experiment}
    We can see that the mean and variance approach some values as we perform
    more experiments
    \begin{figure}
        \includesvg[width=0.8\columnwidth]{"../repeated_experiment_uniform.svg"}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1.2 Gaussian PDF}
    We perform the same experiment, but sample using \jlinl{randn}.
    \begin{jllisting}[gobble=8]
        N = 10_000
        x = randn(N)
        mₓ  = sum(x*1/N)
        σₓ² = sum((x.-mₓ).^2 * 1/N)

        step_size = 0.1
        edges = -5.0:step_size:5.0
        bins = histcounts(x, edges)
        # Normalize PDF
        pdf = bins/sum(bins*step_size)
        theoretical_pdf = (x) -> 1.0/√(2π)*exp(-x.^2 ./ 2)
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1.2 Plotting code}
    We then plot the results in a similar manner as 1.1.
    \begin{jllisting}[gobble=8]
        plot( edges[1:end-1], pdf, seriestype=:steppost,
            label="Estimated PDF", linewidth=2)

        plot!(edges[1:end-1], theoretical_pdf.(edges[1:end-1]),
            label="Theoretical PDF", linewidth=2)

        plot!(legend=:bottom, background_color=:transparent,
            foreground_color=:white)

        title!("PDF of x");xlabel!("Value");
        ylabel!("Probability");
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{1.2 Gaussian PDF plot}    
    \begin{figure}
        \includesvg[width=0.9\columnwidth]{../gaussian_pdf.svg}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1.3 Central Limit Theorem}
    Again, we sample \jlinl{rand} as according to the task. We have
    have the following code.
    \begin{jllisting}[gobble=8]
        N = 10_000
        x = rand(Float64, (12, N))
        ms = mean(x, dims=1)
        
        step_size = 0.01
        edges = 0.0:step_size:1.0
        bins = histcounts(ms, edges)

        pdf = bins/sum(bins*step_size)
        sum(pdf*step_size)

        mₓ = mean(ms)
        σₓ = std(ms)^2

        @printf "mₓ: %.3f" mₓ  # -> mₓ: 0.499
        @printf "σₓ²: %.3f" σₓ # -> σₓ²: 0.007
    \end{jllisting}
\end{frame}
\begin{frame}[fragile]
    We plot the estimated PDF with the following code
    \begin{jllisting}[gobble=8]
        plot(edges[1:end-1], pdf, seriestype=:steppre)
        plot!(background_color=:transparent,foreground_color=:white)
        xlabel!("Value"); ylabel!("Propability")
        title!("PDF of CLT experiment")
        savefig("clt_pdf.svg")
    \end{jllisting}
    \pause
    \begin{figure}
        \includesvg[height=0.6\textheight]{"../clt_pdf.svg"}
    \end{figure}
\end{frame}


\begin{frame}
    \frametitle{Theoretical mean of CTF experiment [$m_z$]}
    We start by noting the random variable for our experiment can be defined as 
    $Z = \frac{1}{12}\sum_{i=1}^{12}X_i$.\\
    \pause
    \medskip 
    Then
    \begin{equation*}
        \E{Z} = \E{\frac{1}{12}\sum_{i=1}^{12}{X_i}} = \frac{1}{12}
        \E{\sum_{i=1}^{12}{X_i}}=\newline \frac{1}{12}\sum_{i=1}^{12}{\E{X_i}}
    \end{equation*} 
    \pause
    As we know that $\E{X_i}=\frac{1}{2}$, we have
    \begin{equation*}
        m_z = \E{Z} = \frac{1}{12}\sum^{12}_{i=1}{\frac{1}{2}}=\frac{1}{2}
    \end{equation*}
    % $\E{(X_i - \bar{X_i})^2} = \frac{1}{12}$
\end{frame}

\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    We want to find the variance $\sigma_x^2 = \E{(Z-\bar{Z})^2}$, and using 
    the definition from the previous slide, we have
    \begin{equation*}
        \E{(Z-\E{Z})^2}=\E{\left(\frac{1}{12}\sum^{12}_{i=1}{X_i} -
        \E{\frac{1}{12}\sum^{12}_{i=1}{X_i}}\right)^2}
    \end{equation*}
    \pause
    We simplify, using
    \begin{equation}
        \E{X+Y} = \E{X} + \E{Y}
    \end{equation}
    and get the following
    \begin{equation*}
        \E{(Z-\E{Z})^2}=\E{\left(\frac{1}{12}\sum^{12}_{i=1}{X_i} 
                - \frac{1}{12}
        \sum^{12}_{i=1}{\E{X_i}}\right)^2}
    \end{equation*}
\end{frame}
\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    We factorize and combine our sums to get
    \begin{equation*}
        \E{(Z-\E{Z})^2} = \left(\frac{1}{12}\right)^2
        \E{\left(\sum^{12}_{i=1}{X_i - \E{X_i}}\right)^2}
    \end{equation*}
    \pause
    Expanding the binomial, we get
    \begin{align*}
        = & \frac{1}{12^2}\sum^{12}_{i=1}{\E{\left(X_i - \E{X_i}\right)^2}} \\
        + & 2 \sum^{j\rightarrow12}_{\substack{i=1\\j=2\\i<j}}
        {\E{(X_i - \E{X_i})(X_j - \E{X_j})}}
    \end{align*}
    Which almost looks good, but there is an ugly term from the binomial
    expansion which ruins the fun.
\end{frame}

\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    There is luckely an easy clean-up for this. We know that our separate 
    observations $X_i$ are all independent. This means that each
    $\left(X_i-\E{X_i}\right) \text{ for } i=1,2,\dots,12$ are independent. \\
    \pause
    We can then use the multiplicative property
    \begin{alertblock}{Multiplicative Property}
        For two independent variables $X$ and $Y$, the expectation of their
        product is equal to the product of the expectation of each variable.
        \begin{equation*}
            \E{XY} = \E{X}\E{Y}
        \end{equation*}
    \end{alertblock}
    \pause
    This allows us to rewrite the additional binomial term as
    \begin{equation*}
        % \E{\left(X_i-\E{X_i}\right)\left(X_j-\E{X_j}\right)} = 
        \E{X_i-\E{X_i}}\cdot\E{X_j-\E{X_j}}
    \end{equation*}
\end{frame} 

\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    We then utilize e.q.(3) on each factor of the binomial term,
    and we see that 
    \begin{gather*}
        \E{X_i-\E{X_i}} = \E{X_i}-\E{\E{X_i}}  =  0 \\
        \Downarrow \\
        2 \sum^{j\rightarrow12}_{\substack{i=1\\j=2\\i<j}}
        { \E{X_i-\E{X_i}}\E{X_j-\E{X_j}}} = 0
    \end{gather*}
    And thus the big bad evil term dissapered.
\end{frame}

\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    Now that we know the additional term equates to 0 and we know
    $\E{\left(X_i - \E{X_i}\right)^2}=\frac{1}{12}$, we insert into our
    equation and get
    values into our equation
    \begin{align*}
        \E{\left(Z-\E{Z}\right)^2} 
        & = \frac{1}{12^2}\sum^{12}_{i=1}
        {\E{\left(X_i - \E{X_i}\right)^2}} \\
        \sigma_z^2  & = \frac{1}{12^2}\sum^{12}_{i=1}
        {\frac{1}{12}} = \frac{1}{144} \approx 0.0069
    \end{align*}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing the theoretical and estimated values}
    We got our theoretical values to be $m_z=0.5,\; \sigma_z^2=0.0069$.
    Comparing this to our result from earlier, we see that our estimated
    values of $m_z=0.499,\; \sigma_z^2=0.007$ is pretty much spot on. \\
    \medskip
    We then plot a gaussian curve over our previous plot with these values
    using the following code
    \begin{jllisting}[gobble=8]
        theoretical_pdf = (x) -> 1.0/√(2π*σₓ²)*exp(-(x-mₓ).^2 ./ (2σₓ²))
        plot(edges[1:end-1], pdf, seriestype=:steppost, label="Estimate")
        plot!(edges[1:end-1], theoretical_pdf.(edges[1:end-1]), label="Theoretical")
        plot!(background_color=:transparent,foreground_color=:white)
        xlabel!("Value"); ylabel!("Propability")
        title!("PDF of CLT experiment")
        savefig("clt_pdf_theoretical.svg")
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{Plot of theoretical and estimated PDF}
    \begin{figure}
        \includesvg[width=0.9\columnwidth]{"../clt_pdf_theoretical.svg"}
    \end{figure}
\end{frame}

\end{document}
